# Отчет по проекту Induction Heads

## 1. Введение
### Описание задачи
В этом проекте пытаюсь повторить статью антропиков про Induction Heads.

### Цели проекта
1. Обучить две разные модели (2*attn и conv-attn) и выявить Induction Heads.

## 2. Методология
### 2.1 Архитектура модели
- Первая модель архитектурно двухслойная, первый слой Conv1d, второй слой causal self-attention.
- Вторая модель также двухслойная, только теперь оба слоя это causal self-attention

### 2.2 Процесс обучения
В качестве датасета для обучения взял openwebtext, так как там 9млрд токенов, а в статье указывалось, что Induction Heads выявляются примерно на 2.5-5млрд токенах. Поэтому предложенные датасеты пришлось отбросить, так как в TinyStories, например, всего 300млн токенов. Параметры моделей (размерность, количество голов) взяты в основном из самой первой статьи по трансформерам. Параметры обучения все базовые, взяты с туториалов HF. Слой Attention решил написать сам, потому что так интереснее. 

### 2.3 Проблемы при обучении
Было несколько серьезных проблем:
- Сначала моделька показывала довольно плохие attention карты, выглядело так, что она вообще не обращала внимание ни на какие токены, оказалось, что я забыл добавить positional embeddings и сдвинуть токены на один вправо (мне почему-то казалось, что transformers это делают под капотом, но покопался в сурсах, оказывается это происходит не на уровне Trainerm а раньше в самой модели), сразу же дописал и все стало ок.
- Очень долго шло полное обучение модели, на одной А100 занимало около 60 часов. Добавил torch.compile, время сократилось до 10 часов.


## 3. Результаты

<img src="assets/coat_h4_1.png" alt="behavioral score" width="500"/>
<img src="assets/coat_h4_2.png" alt="behavioral score" width="500"/>
<img src="assets/coat_h4_3.png" alt="behavioral score" width="500"/>
Три картинки подряд для первой модели, визуализация внимания 4 головы. Даже по самому паттерну видно наличие induction head. Результаты в целом это подтверждают, видим, что, например, текущий токен "D" в слове "Dursley" смотрит назад не просто на токен "D", но сразу на "urs", что явно указывает на наличие Induction Head.

<img src="assets/atat_full.png" alt="behavioral score" width="500"/>
К сожалению, в модели с двумя self-attention не получилось выявить Induction Heads, скорее всего, я сделал что-то не так в архитектурном плане, постараюсь отдебажить и долить в репо.

<img src="assets/conv_weights.png" alt="behavioral score" width="500"/>
На визуализации видим веса conv слоя, тут отлично видно, что веса получаются "размазанными", что и должно было получиться теоретически, если бы веса повторяли концепцию smeared keys.

## 4. Список литературы
1. [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
2. [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)